{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17757522-3108-429c-bd81-336d1bb7729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerias necesarias\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.stat import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "## Información para acceder a Azure Storage\n",
    "blob_account_name = \"\"\n",
    "blob_container_name = \"\"\n",
    "blob_origin_path = \"silver\"\n",
    "blob_destination_path = \"Golden\"\n",
    "blob_sas_token = r\"\"\n",
    "\n",
    "spark.conf.set(f'fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net', blob_sas_token)\n",
    "\n",
    "# Configuración de SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AzureBlobStorage\") \\\n",
    "    .config(f\"fs.azure.account.key.{blob_account_name}.blob.core.windows.net\", \"\") \\\n",
    "    .getOrCreate()\n",
    "wasbs_path = f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/\"\n",
    "data=spark.read.parquet(wasbs_path+blob_origin_path)\n",
    "\n",
    "  \n",
    "display(data.limit(15))\n",
    "\n",
    "\n",
    "\n",
    "column_names = data.schema.names\n",
    "print(column_names)\n",
    "\n",
    "display(data.groupBy(\"Label\").agg(F.first(\"Week_Day\").alias(\"dia del ataque\")))\n",
    "\n",
    "## Análisis de Protocolos más Utilizados por Día de la Semana\n",
    "\n",
    "\n",
    "\n",
    "windowSpec = Window().partitionBy(\"Week_Day\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "display(data.groupBy(\"Week_Day\", \"Label\", \"Protocol\").count().withColumn(\n",
    "    \"rank\", F.row_number().over(windowSpec)\n",
    ").filter(\"rank == 1\").drop(\"rank\").withColumnRenamed(\"Protocol\", \"protocolo_mas_usado\"))\n",
    "\n",
    "# Visualización de Dispersión entre Week_Day y Label\n",
    "\n",
    "\n",
    "\n",
    "# Convertir el DataFrame de Spark a Pandas\n",
    "pandas_df = data.select(\"Week_Day\", \"Label\").toPandas()\n",
    "\n",
    "# Crear el gráfico de dispersión\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(pandas_df['Week_Day'], pandas_df['Label'], alpha=0.5)\n",
    "plt.title('Gráfico de Dispersión entre Week_Day y Label')\n",
    "plt.xlabel('Week_Day')\n",
    "plt.ylabel('Label')\n",
    "plt.show()\n",
    "\n",
    "## Histograma de Frecuencias por Día de la Semana\n",
    "# Filtra los datos para incluir solo el rango de Timestamp deseado (por ejemplo, para el año 2018)\n",
    "start_date = \"2018-01-01\"\n",
    "end_date = \"2018-12-31\"\n",
    "filtered_data = data.filter((F.col(\"Timestamp\") >= start_date) & (F.col(\"Timestamp\") <= end_date))\n",
    "\n",
    "# Encuentra los 2 Label más comunes\n",
    "top_labels = (\n",
    "    filtered_data.groupBy(\"Label\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .limit(3)\n",
    "    .select(\"Label\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Filtra los datos para incluir solo los 2 Label más comunes\n",
    "filtered_data = filtered_data.filter(F.col(\"Label\").isin(top_labels))\n",
    "\n",
    "# Agrupa por week_day y cuenta la frecuencia de cada Label\n",
    "histogram_data = (\n",
    "    filtered_data.groupBy(\"Week_Day\", \"Label\")\n",
    "    .count()\n",
    "    .groupBy(\"Week_Day\")\n",
    "    .pivot(\"Label\")\n",
    "    .agg(F.first(\"count\"))\n",
    "    .na.fill(0)\n",
    ")\n",
    "\n",
    "# Convierte a Pandas DataFrame para visualización con matplotlib\n",
    "pd_data = histogram_data.toPandas()\n",
    "\n",
    "# Grafico de histograma\n",
    "bar_width = 0.35\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "labels = pd_data[\"Week_Day\"]\n",
    "bar1 = ax.bar(labels, pd_data[top_labels[0]], bar_width, label=top_labels[0])\n",
    "bar2 = ax.bar(labels, pd_data[top_labels[1]], bar_width, label=top_labels[1], bottom=pd_data[top_labels[0]])\n",
    "\n",
    "ax.set_title(\"Histograma de Frecuencias por Día de la Semana\")\n",
    "ax.set_xlabel(\"Week_Day\")\n",
    "ax.set_ylabel(\"Frecuencia\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Exploremos nulls\n",
    "# Contar los valores nulos en cada columna\n",
    "null_counts = [(col_name, data.filter(col(col_name).isNull()).count()) for col_name in data.columns if data.filter(col(col_name).isNull()).count() > 0]\n",
    "\n",
    "# Imprimir los recuentos\n",
    "for col_name, count in null_counts:\n",
    "    print(f\"La columna '{col_name}' tiene {count} valores nulos.\")\n",
    "\n",
    "# Identifiquemos valores **atipicos**\n",
    "\n",
    "# Convierte el DataFrame de PySpark a un DataFrame de pandas\n",
    "pandas_df = data.select('Flow_Duration').toPandas()\n",
    "\n",
    "# Crea el boxplot con seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Flow_Duration', data=pandas_df)\n",
    "plt.title('Boxplot de Flow_Duration')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Analisis de Correlación y Filtrado de datos:\n",
    "# Convertir las columnas \"Dst_Port\" y \"Protocol\" al tipo de datos \"integer\"\n",
    "Corr = data.withColumn(\"Dst_Port\", col(\"Dst_Port\").cast(\"integer\")).withColumn(\"Protocol\", col(\"Protocol\").cast(\"integer\"))\n",
    "\n",
    "# Seleccionar las columnas deseadas\n",
    "dfCorr = Corr.select(column_names)\n",
    "\n",
    "# Indexar la columna \"Label\" y transformarla en una columna numérica llamada \"Ataque\"\n",
    "indexed_df = StringIndexer(inputCol=\"Label\", outputCol=\"Ataque\").fit(dfCorr).transform(dfCorr)\n",
    "\n",
    "new=['Dst_Port', 'Protocol', 'Flow_Duration', 'Tot_Fwd_Pkts', 'Tot_Bwd_Pkts', 'TotLen_Fwd_Pkts', 'TotLen_Bwd_Pkts', 'Fwd_Pkt_Len_Max', 'Fwd_Pkt_Len_Min', 'Fwd_Pkt_Len_Mean', 'Fwd_Pkt_Len_Std', 'Bwd_Pkt_Len_Max', 'Bwd_Pkt_Len_Min', 'Bwd_Pkt_Len_Mean', 'Bwd_Pkt_Len_Std', 'Flow_Byts/s', 'Flow_Pkts/s', 'Flow_IAT_Mean', 'Flow_IAT_Std', 'Flow_IAT_Max', 'Flow_IAT_Min', 'Fwd_IAT_Tot', 'Fwd_IAT_Mean', 'Fwd_IAT_Std', 'Fwd_IAT_Max', 'Fwd_IAT_Min', 'Bwd_IAT_Tot', 'Bwd_IAT_Mean', 'Bwd_IAT_Std', 'Bwd_IAT_Max', 'Bwd_IAT_Min', 'Fwd_PSH_Flags', 'Bwd_PSH_Flags', 'Fwd_URG_Flags', 'Bwd_URG_Flags', 'Fwd_Header_Len', 'Bwd_Header_Len', 'Fwd_Pkts/s', 'Bwd_Pkts/s', 'Pkt_Len_Min', 'Pkt_Len_Max', 'Pkt_Len_Mean', 'Pkt_Len_Std', 'Pkt_Len_Var', 'FIN_Flag_Cnt', 'SYN_Flag_Cnt', 'RST_Flag_Cnt', 'PSH_Flag_Cnt', 'ACK_Flag_Cnt', 'URG_Flag_Cnt', 'CWE_Flag_Count', 'ECE_Flag_Cnt', 'Down/Up_Ratio', 'Pkt_Size_Avg', 'Fwd_Seg_Size_Avg', 'Bwd_Seg_Size_Avg', 'Fwd_Byts/b_Avg', 'Fwd_Pkts/b_Avg', 'Fwd_Blk_Rate_Avg', 'Bwd_Byts/b_Avg', 'Bwd_Pkts/b_Avg', 'Bwd_Blk_Rate_Avg', 'Subflow_Fwd_Pkts', 'Subflow_Fwd_Byts', 'Subflow_Bwd_Pkts', 'Subflow_Bwd_Byts', 'Init_Fwd_Win_Byts', 'Init_Bwd_Win_Byts', 'Fwd_Act_Data_Pkts', 'Fwd_Seg_Size_Min', 'Active_Mean', 'Active_Std', 'Active_Max', 'Active_Min', 'Idle_Mean', 'Idle_Std', 'Idle_Max', 'Idle_Min', 'Ataque']\n",
    "df = indexed_df.select(new)\n",
    "\n",
    "# Lista de condiciones para cada columna\n",
    "conditions = [~col(c).isin(float(\"nan\"), float(\"inf\"), float(\"-inf\")) for c in new]\n",
    "\n",
    "# Uso de reduce para combinar todas las condiciones mediante el operador AND (&)\n",
    "combined_condition = reduce(lambda x, y: x & y, conditions)\n",
    "\n",
    "# Filtrar el DataFrame utilizando la condición combinada\n",
    "filtered_df = df.filter(combined_condition)\n",
    "\n",
    "# Mostrar el recuento de filas resultantes\n",
    "filtered_count = df.count()\n",
    "print(f\"Recuento de filas después de aplicar el filtro: {filtered_count}\")\n",
    "\n",
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=filtered_df.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(filtered_df).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col)\n",
    "\n",
    "matrix.collect()[0][\"pearson({})\".format(vector_col)].values\n",
    "\n",
    "matrix = Correlation.corr(df_vector, 'corr_features').collect()[0][0] \n",
    "corr_matrix = matrix.toArray().tolist() \n",
    "corr_matrix_df = pd.DataFrame(data=corr_matrix, columns = new, index=new) \n",
    "\n",
    "## Graficamos la matriz de correlacion\n",
    "\n",
    "corr_matrix_df .style.background_gradient(cmap='summer').set_precision(2)\n",
    "\n",
    "\n",
    "# Nueva data \n",
    "# Obtiene la lista de nombres de las columnas del DataFrame 'df'\n",
    "column_names = df.columns\n",
    "\n",
    "# Filtra el DataFrame 'df' utilizando una condición combinada y almacena el resultado en 'Data'\n",
    "Data = df.filter(combined_condition)\n",
    "\n",
    "# Define una lista de nuevas columnas que se seleccionarán del DataFrame 'Data'\n",
    "new_columns = [\n",
    "    \"Protocol\",\n",
    "    \"Fwd_Pkt_Len_Max\",\n",
    "    \"Fwd_Pkt_Len_Min\",\n",
    "    \"Fwd_Pkt_Len_Mean\",\n",
    "    \"Fwd_Pkt_Len_Std\",\n",
    "    \"Bwd_Pkt_Len_Max\",\n",
    "    \"Bwd_Pkt_Len_Min\",\n",
    "    \"Bwd_Pkt_Len_Mean\",\n",
    "    \"Flow_Pkts/s\",\n",
    "    \"Fwd_Pkts/s\",\n",
    "    \"Bwd_Pkts/s\",\n",
    "    \"Pkt_Len_Min\",\n",
    "    \"Pkt_Len_Max\",\n",
    "    \"Pkt_Len_Mean\",\n",
    "    \"Pkt_Len_Std\",\n",
    "    \"PSH_Flag_Cnt\",\n",
    "    \"Pkt_Size_Avg\",\n",
    "    \"Fwd_Seg_Size_Avg\",\n",
    "    \"Bwd_Seg_Size_Avg\",\n",
    "    \"Init_Fwd_Win_Byts\",\n",
    "    \"Init_Bwd_Win_Byts\",\n",
    "    \"Fwd_Seg_Size_Min\",\n",
    "    \"Ataque\"\n",
    "]\n",
    "\n",
    "# Selecciona las nuevas columnas del DataFrame 'Data'\n",
    "Data = Data.select(new_columns)\n",
    "\n",
    "# Muestra el DataFrame 'Data'\n",
    "display(Data)\n",
    "\n",
    "\n",
    "# Guardar en formato delta\n",
    "\n",
    "Data.write.format(\"delta\").mode(\"append\").partitionBy(\"Ataque\").save(wasbs_path + blob_destination_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
