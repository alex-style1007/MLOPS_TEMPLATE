{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce623a0-51f4-4b0f-aa91-6da0770eec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *   # Importación de funciones de Spark SQL\n",
    "\n",
    "# Información de acceso a Azure Storage\n",
    "blob_account_name = \"nombre de tu cuenta\"\n",
    "blob_container_name = \"nombre del contenedor\"\n",
    "blob_origin_path = \"aws\" #carpeta donde guardamos los datos de aws\n",
    "blob_destination_path = \"bronze\" #carpeta donde guardaremos los datos en la capa bronze\n",
    "blob_sas_token = r\"Sas token\"\n",
    "\n",
    "# Configuración de la conexión utilizando el token SAS\n",
    "spark.conf.set(f'fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net', blob_sas_token)\n",
    "\n",
    "# Configuración de SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AzureBlobStorage\") \\\n",
    "    .config(f\"fs.azure.account.key.{blob_account_name}.blob.core.windows.net\", \"\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Construcción de la ruta de acceso al Blob Storage\n",
    "wasbs_path = f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/\"\n",
    "\n",
    "# Lectura de datos desde el Blob Storage en formato Parquet y filtrado de registros\n",
    "data = spark.read.parquet(wasbs_path + blob_origin_path).where(\"Label!='Label'\")\n",
    "\n",
    "# 1. Exploración de datos aun de una forma muy abstracta\n",
    "\n",
    "display(data)\n",
    "\n",
    "data.printSchema()\n",
    "\n",
    "display(data.describe().show())\n",
    "\n",
    "#2. Generar un esquema para los datos\n",
    "\n",
    "# definicion del equema original \n",
    "Original_schema = StructType([\n",
    "    StructField('Dst Port', IntegerType(), True),\n",
    "    StructField('Protocol', IntegerType(), True),\n",
    "    StructField('Timestamp', TimestampType(), True),\n",
    "    StructField('Flow Duration', IntegerType(), True),\n",
    "    StructField('Tot Fwd Pkts', IntegerType(), True),\n",
    "    StructField('Tot Bwd Pkts', IntegerType(), True),\n",
    "    StructField('TotLen Fwd Pkts', DoubleType(), True),\n",
    "    StructField('TotLen Bwd Pkts', DoubleType(), True),\n",
    "    StructField('Fwd Pkt Len Max', IntegerType(), True),\n",
    "    StructField('Fwd Pkt Len Min', IntegerType(), True),\n",
    "    StructField('Fwd Pkt Len Mean', DoubleType(), True),\n",
    "    StructField('Fwd Pkt Len Std', DoubleType(), True),\n",
    "    StructField('Bwd Pkt Len Max', IntegerType(), True),\n",
    "    StructField('Bwd Pkt Len Min', IntegerType(), True),\n",
    "    StructField('Bwd Pkt Len Mean', DoubleType(), True),\n",
    "    StructField('Bwd Pkt Len Std', DoubleType(), True),\n",
    "    StructField('Flow Byts/s', DoubleType(), True),\n",
    "    StructField('Flow Pkts/s', DoubleType(), True),\n",
    "    StructField('Flow IAT Mean', DoubleType(), True),\n",
    "    StructField('Flow IAT Std', DoubleType(), True),\n",
    "    StructField('Flow IAT Max', DoubleType(), True),\n",
    "    StructField('Flow IAT Min', DoubleType(), True),\n",
    "    StructField('Fwd IAT Tot', DoubleType(), True),\n",
    "    StructField('Fwd IAT Mean', DoubleType(), True),\n",
    "    StructField('Fwd IAT Std', DoubleType(), True),\n",
    "    StructField('Fwd IAT Max', DoubleType(), True),\n",
    "    StructField('Fwd IAT Min', DoubleType(), True),\n",
    "    StructField('Bwd IAT Tot', DoubleType(), True),\n",
    "    StructField('Bwd IAT Mean', DoubleType(), True),\n",
    "    StructField('Bwd IAT Std', DoubleType(), True),\n",
    "    StructField('Bwd IAT Max', DoubleType(), True),\n",
    "    StructField('Bwd IAT Min', DoubleType(), True),\n",
    "    StructField('Fwd PSH Flags', IntegerType(), True),\n",
    "    StructField('Bwd PSH Flags', IntegerType(), True),\n",
    "    StructField('Fwd URG Flags', IntegerType(), True),\n",
    "    StructField('Bwd URG Flags', IntegerType(), True),\n",
    "    StructField('Fwd Header Len', IntegerType(), True),\n",
    "    StructField('Bwd Header Len', IntegerType(), True),\n",
    "    StructField('Fwd Pkts/s', DoubleType(), True),\n",
    "    StructField('Bwd Pkts/s', DoubleType(), True),\n",
    "    StructField('Pkt Len Min', IntegerType(), True),\n",
    "    StructField('Pkt Len Max', IntegerType(), True),\n",
    "    StructField('Pkt Len Mean', DoubleType(), True),\n",
    "    StructField('Pkt Len Std', DoubleType(), True),\n",
    "    StructField('Pkt Len Var', DoubleType(), True),\n",
    "    StructField('FIN Flag Cnt', IntegerType(), True),\n",
    "    StructField('SYN Flag Cnt', IntegerType(), True),\n",
    "    StructField('RST Flag Cnt', IntegerType(), True),\n",
    "    StructField('PSH Flag Cnt', IntegerType(), True),\n",
    "    StructField('ACK Flag Cnt', IntegerType(), True),\n",
    "    StructField('URG Flag Cnt', IntegerType(), True),\n",
    "    StructField('CWE Flag Count', IntegerType(), True),\n",
    "    StructField('ECE Flag Cnt', IntegerType(), True),\n",
    "    StructField('Down/Up Ratio', DoubleType(), True),\n",
    "    StructField('Pkt Size Avg', DoubleType(), True),\n",
    "    StructField('Fwd Seg Size Avg', DoubleType(), True),\n",
    "    StructField('Bwd Seg Size Avg', DoubleType(), True),\n",
    "    StructField('Fwd Byts/b Avg', DoubleType(), True),\n",
    "    StructField('Fwd Pkts/b Avg', DoubleType(), True),\n",
    "    StructField('Fwd Blk Rate Avg', DoubleType(), True),\n",
    "    StructField('Bwd Byts/b Avg', DoubleType(), True),\n",
    "    StructField('Bwd Pkts/b Avg', DoubleType(), True),\n",
    "    StructField('Bwd Blk Rate Avg', DoubleType(), True),\n",
    "    StructField('Subflow Fwd Pkts', IntegerType(), True),\n",
    "    StructField('Subflow Fwd Byts', IntegerType(), True),\n",
    "    StructField('Subflow Bwd Pkts', IntegerType(), True),\n",
    "    StructField('Subflow Bwd Byts', IntegerType(), True),\n",
    "    StructField('Init Fwd Win Byts', IntegerType(), True),\n",
    "    StructField('Init Bwd Win Byts', IntegerType(), True),\n",
    "    StructField('Fwd Act Data Pkts', IntegerType(), True),\n",
    "    StructField('Fwd Seg Size Min', IntegerType(), True),\n",
    "    StructField('Active Mean', DoubleType(), True),\n",
    "    StructField('Active Std', DoubleType(), True),\n",
    "    StructField('Active Max', DoubleType(), True),\n",
    "    StructField('Active Min', DoubleType(), True),\n",
    "    StructField('Idle Mean', DoubleType(), True),\n",
    "    StructField('Idle Std', DoubleType(), True),\n",
    "    StructField('Idle Max', DoubleType(), True),\n",
    "    StructField('Idle Min', DoubleType(), True),\n",
    "    StructField('Label', StringType(), True),\n",
    "    StructField('date', StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Lectura de los datos utilizando el esquema original\n",
    "data = spark.read.schema(Original_schema).parquet(wasbs_path + blob_origin_path)\n",
    "\n",
    "# Conversión del campo 'Timestamp' al formato adecuado\n",
    "data = data.withColumn(\"Timestamp\", to_timestamp(data[\"Timestamp\"], \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "dataTypes = {'StringType()': 'string', 'DoubleType()': 'double', 'IntegerType()': 'integer', 'TimestampType()': 'timestamp'} #diccionario con tipos de datos\n",
    "data = data.select([col(field.name).cast(dataTypes[str(field.dataType)]).alias(field.name) for field in Original_schema.fields]) #transformacion de los datos\n",
    "\n",
    "#3. Guardar datos en databricks\n",
    "# Directorio raíz para los datos de destino\n",
    "destDataDirRoot = f\"/mnt/bigdata/nombre de usuario/carpeta raiz/bronze/tmpdata/\"\n",
    "\n",
    "# Escribir los datos en formato Parquet, con modo de anexado y particionado por la columna 'date'\n",
    "data.write.format(\"parquet\").mode(\"append\").partitionBy(\"date\").save(destDataDirRoot)\n",
    "\n",
    "# Mostrar los archivos en el directorio de destino\n",
    "display(dbutils.fs.ls(destDataDirRoot))\n",
    "\n",
    "#4. Lectura de datos con el equema corregido\n",
    "\n",
    "# Lectura de los datos en formato Parquet, aplicando el esquema original y configurando opciones\n",
    "data = spark.read.format(\"parquet\").schema(Original_schema).option(\"header\", True).load(destDataDirRoot).cache()\n",
    "\n",
    "# Mostrar los datos filtrados por la columna 'date' con valor 'Friday-23-02-2018'\n",
    "display(data.where(\"date='Friday-23-02-2018'\"))\n",
    "\n",
    "#5. Guardar los datos en azure\n",
    "\n",
    "# Escribir los datos en formato Parquet, con modo de anexado y particionado por la columna 'date'\n",
    "data.write.format(\"parquet\").mode(\"append\").partitionBy(\"date\").save(wasbs_path + blob_destination_path)\n",
    "\n",
    "# Mostrar los archivos en el directorio de destino en Azure Blob Storage\n",
    "display(dbutils.fs.ls(wasbs_path + blob_destination_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
